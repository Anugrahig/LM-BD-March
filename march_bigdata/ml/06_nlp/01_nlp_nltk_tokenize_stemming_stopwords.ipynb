{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a69e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e706ad",
   "metadata": {},
   "source": [
    "###### nltk : natural langage toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ee3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk:\n",
      "\n",
      "NAME\n",
      "    nltk\n",
      "\n",
      "DESCRIPTION\n",
      "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
      "    for Natural Language Processing.  A free online book is available.\n",
      "    (If you use the library for academic research, please cite the book.)\n",
      "    \n",
      "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
      "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
      "    https://www.nltk.org/book/\n",
      "    \n",
      "    isort:skip_file\n",
      "    \n",
      "    @version: 3.7\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    app (package)\n",
      "    book\n",
      "    ccg (package)\n",
      "    chat (package)\n",
      "    chunk (package)\n",
      "    classify (package)\n",
      "    cli\n",
      "    cluster (package)\n",
      "    collections\n",
      "    collocations\n",
      "    compat\n",
      "    corpus (package)\n",
      "    data\n",
      "    decorators\n",
      "    downloader\n",
      "    draw (package)\n",
      "    featstruct\n",
      "    grammar\n",
      "    help\n",
      "    inference (package)\n",
      "    internals\n",
      "    jsontags\n",
      "    lazyimport\n",
      "    lm (package)\n",
      "    metrics (package)\n",
      "    misc (package)\n",
      "    parse (package)\n",
      "    probability\n",
      "    sem (package)\n",
      "    sentiment (package)\n",
      "    stem (package)\n",
      "    tag (package)\n",
      "    tbl (package)\n",
      "    test (package)\n",
      "    text\n",
      "    tgrep\n",
      "    tokenize (package)\n",
      "    toolbox\n",
      "    translate (package)\n",
      "    tree (package)\n",
      "    treeprettyprinter\n",
      "    treetransforms\n",
      "    twitter (package)\n",
      "    util\n",
      "    wsd\n",
      "\n",
      "SUBMODULES\n",
      "    agreement\n",
      "    aline\n",
      "    api\n",
      "    arlstem\n",
      "    arlstem2\n",
      "    association\n",
      "    bleu_score\n",
      "    bllip\n",
      "    boxer\n",
      "    brill\n",
      "    brill_trainer\n",
      "    casual\n",
      "    chart\n",
      "    chrf_score\n",
      "    cistem\n",
      "    confusionmatrix\n",
      "    corenlp\n",
      "    crf\n",
      "    decisiontree\n",
      "    dependencygraph\n",
      "    destructive\n",
      "    discourse\n",
      "    distance\n",
      "    drt\n",
      "    earleychart\n",
      "    evaluate\n",
      "    featurechart\n",
      "    gale_church\n",
      "    gdfa\n",
      "    gleu_score\n",
      "    glue\n",
      "    hmm\n",
      "    hunpos\n",
      "    ibm1\n",
      "    ibm2\n",
      "    ibm3\n",
      "    ibm4\n",
      "    ibm5\n",
      "    ibm_model\n",
      "    immutable\n",
      "    isri\n",
      "    lancaster\n",
      "    legality_principle\n",
      "    lfg\n",
      "    linearlogic\n",
      "    logic\n",
      "    mace\n",
      "    malt\n",
      "    mapping\n",
      "    maxent\n",
      "    megam\n",
      "    meteor_score\n",
      "    mwe\n",
      "    naivebayes\n",
      "    nist_score\n",
      "    nonprojectivedependencyparser\n",
      "    paice\n",
      "    parented\n",
      "    parsing\n",
      "    pchart\n",
      "    perceptron\n",
      "    phrase_based\n",
      "    porter\n",
      "    positivenaivebayes\n",
      "    prettyprinter\n",
      "    probabilistic\n",
      "    projectivedependencyparser\n",
      "    prover9\n",
      "    punkt\n",
      "    recursivedescent\n",
      "    regexp\n",
      "    relextract\n",
      "    repp\n",
      "    resolution\n",
      "    ribes_score\n",
      "    rslp\n",
      "    rte_classify\n",
      "    scikitlearn\n",
      "    scores\n",
      "    segmentation\n",
      "    senna\n",
      "    sequential\n",
      "    sexpr\n",
      "    shiftreduce\n",
      "    simple\n",
      "    snowball\n",
      "    sonority_sequencing\n",
      "    spearman\n",
      "    stack_decoder\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    tableau\n",
      "    tadm\n",
      "    textcat\n",
      "    texttiling\n",
      "    tnt\n",
      "    toktok\n",
      "    transforms\n",
      "    transitionparser\n",
      "    treebank\n",
      "    viterbi\n",
      "    weka\n",
      "    wordnet\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # FIXME:  override any accidentally imported demo, see https://github.com/nltk/nltk/issues/2116\n",
      "    \n",
      "    tee(iterable, n=2, /)\n",
      "        Returns a tuple of n independent iterators.\n",
      "\n",
      "DATA\n",
      "    RUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptro...\n",
      "    SLASH = *slash*\n",
      "    TYPE = *type*\n",
      "    __author_email__ = 'nltk.team@gmail.com'\n",
      "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
      "    __copyright__ = 'Copyright (C) 2001-2022 NLTK Project.\\n\\nDistribut......\n",
      "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
      "    __license__ = 'Apache License, Version 2.0'\n",
      "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ...in...\n",
      "    __maintainer__ = 'NLTK Team'\n",
      "    __maintainer_email__ = 'nltk.team@gmail.com'\n",
      "    __url__ = 'https://www.nltk.org/'\n",
      "    app = <LazyModule 'nltk.nltk.app'>\n",
      "    chat = <LazyModule 'nltk.nltk.chat'>\n",
      "    corpus = <LazyModule 'nltk.nltk.corpus'>\n",
      "    infile = <_io.TextIOWrapper name='/home/anugrah/anaconda3...packages/n...\n",
      "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
      "    toolbox = <LazyModule 'nltk.nltk.toolbox'>\n",
      "    version_file = '/home/anugrah/anaconda3/lib/python3.10/site-packages/n...\n",
      "\n",
      "VERSION\n",
      "    3.7\n",
      "\n",
      "AUTHOR\n",
      "    NLTK Team\n",
      "\n",
      "FILE\n",
      "    /home/anugrah/anaconda3/lib/python3.10/site-packages/nltk/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6038f",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;> Identification of Stop words <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38508914",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;\" >Identification of Stopwords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0081c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to download stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86664e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anugrah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw=stopwords.words(\"english\")\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb7c9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a42601",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;\" >Tokenization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df31bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anugrah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '(',\n",
       " 'NLTK',\n",
       " ')',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'Python',\n",
       " 'library']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "sentance=\"The Natural Language Toolkit (NLTK) is an open source Python library\"\n",
    "# word_tokenize\n",
    "\n",
    "tokens=nltk.word_tokenize(sentance,language=\"english\",preserve_line=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef3b019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '(',\n",
       " 'NLTK',\n",
       " ')',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'Python',\n",
       " 'library']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1dc1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem\n",
      "Ipsum\n",
      "is\n",
      "simply\n",
      "dummy\n",
      "text\n",
      "of\n",
      "the\n",
      "printing\n",
      "and\n",
      "typesetting\n",
      "industry.\n",
      "Lorem\n",
      "Ipsum\n",
      "has\n",
      "been\n",
      "the\n",
      "industry's\n",
      "standard\n",
      "dummy\n",
      "text\n",
      "ever\n",
      "since\n",
      "the\n",
      "1500s,\n",
      "when\n",
      "an\n",
      "unknown\n",
      "printer\n",
      "took\n",
      "a\n",
      "galley\n",
      "of\n",
      "type\n",
      "and\n",
      "scrambled\n",
      "it\n",
      "to\n",
      "make\n",
      "a\n",
      "type\n",
      "specimen\n",
      "book.\n",
      "It\n",
      "has\n",
      "survived\n",
      "not\n",
      "only\n",
      "five\n",
      "centuries,\n",
      "but\n",
      "also\n",
      "the\n",
      "leap\n",
      "into\n",
      "electronic\n",
      "typesetting,\n",
      "remaining\n",
      "essentially\n",
      "unchanged.\n",
      "It\n",
      "was\n",
      "popularised\n",
      "in\n",
      "the\n",
      "1960s\n",
      "with\n",
      "the\n",
      "release\n",
      "of\n",
      "Letraset\n",
      "sheets\n",
      "containing\n",
      "Lorem\n",
      "Ipsum\n",
      "passages,\n",
      "and\n",
      "more\n",
      "recently\n",
      "with\n",
      "desktop\n",
      "publishing\n",
      "software\n",
      "like\n",
      "Aldus\n",
      "PageMaker\n",
      "including\n",
      "versions\n",
      "of\n",
      "Lorem\n",
      "Ipsum.\n"
     ]
    }
   ],
   "source": [
    "for i in sentance1.split():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39adb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance1=\"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
    " Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "  when an unknown printer took a galley of type and scrambled it to make a type specimen book. \n",
    "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\n",
    " It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages,\n",
    "  and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01929fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry.',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " \"industry's\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'the',\n",
       " '1500s,',\n",
       " 'when',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'a',\n",
       " 'galley',\n",
       " 'of',\n",
       " 'type',\n",
       " 'and',\n",
       " 'scrambled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'survived',\n",
       " 'not',\n",
       " 'only',\n",
       " 'five',\n",
       " 'centuries,',\n",
       " 'but',\n",
       " 'also',\n",
       " 'the',\n",
       " 'leap',\n",
       " 'into',\n",
       " 'electronic',\n",
       " 'typesetting,',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'popularised',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'with',\n",
       " 'the',\n",
       " 'release',\n",
       " 'of',\n",
       " 'Letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'passages,',\n",
       " 'and',\n",
       " 'more',\n",
       " 'recently',\n",
       " 'with',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'Aldus',\n",
       " 'PageMaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited = [ i  for i in sentance1.split() if i not in tokens ]\n",
    "splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fbe9414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorem',\n",
       " 'ipsum',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'printing',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'industry',\n",
       " \"'s\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " '1500s',\n",
       " ',',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'galley',\n",
       " 'type',\n",
       " 'scrambled',\n",
       " 'make',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " '.',\n",
       " 'it',\n",
       " 'survived',\n",
       " 'five',\n",
       " 'centuries',\n",
       " ',',\n",
       " 'also',\n",
       " 'leap',\n",
       " 'electronic',\n",
       " 'typesetting',\n",
       " ',',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged',\n",
       " '.',\n",
       " 'it',\n",
       " 'popularised',\n",
       " '1960s',\n",
       " 'release',\n",
       " 'letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'passages',\n",
       " ',',\n",
       " 'recently',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'aldus',\n",
       " 'pagemaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punket\")\n",
    "\n",
    "sw1=stopwords.words(\"english\")\n",
    "tokens1=nltk.word_tokenize(sentance1)\n",
    "tokens2=[i.lower() for i in tokens1 if i not in sw1 ]\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c7ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/anugrah/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8b55eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lorem', 'NNP'),\n",
       " ('Ipsum', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('simply', 'RB'),\n",
       " ('dummy', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('printing', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('typesetting', 'NN'),\n",
       " ('industry', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Lorem', 'NNP'),\n",
       " ('Ipsum', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('been', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('industry', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('standard', 'JJ'),\n",
       " ('dummy', 'NN'),\n",
       " ('text', 'NN'),\n",
       " ('ever', 'RB'),\n",
       " ('since', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('1500s', 'CD'),\n",
       " (',', ','),\n",
       " ('when', 'WRB'),\n",
       " ('an', 'DT'),\n",
       " ('unknown', 'JJ'),\n",
       " ('printer', 'NN'),\n",
       " ('took', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('galley', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('type', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('scrambled', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('type', 'NN'),\n",
       " ('specimen', 'NNS'),\n",
       " ('book', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('has', 'VBZ'),\n",
       " ('survived', 'VBN'),\n",
       " ('not', 'RB'),\n",
       " ('only', 'RB'),\n",
       " ('five', 'CD'),\n",
       " ('centuries', 'NNS'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('also', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('leap', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('electronic', 'JJ'),\n",
       " ('typesetting', 'NN'),\n",
       " (',', ','),\n",
       " ('remaining', 'VBG'),\n",
       " ('essentially', 'RB'),\n",
       " ('unchanged', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('popularised', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('1960s', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('release', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Letraset', 'NNP'),\n",
       " ('sheets', 'NNS'),\n",
       " ('containing', 'VBG'),\n",
       " ('Lorem', 'NNP'),\n",
       " ('Ipsum', 'NNP'),\n",
       " ('passages', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('more', 'RBR'),\n",
       " ('recently', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('desktop', 'NN'),\n",
       " ('publishing', 'NN'),\n",
       " ('software', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('Aldus', 'NNP'),\n",
       " ('PageMaker', 'NNP'),\n",
       " ('including', 'VBG'),\n",
       " ('versions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('Lorem', 'NNP'),\n",
       " ('Ipsum', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post tage : nnp :proper noun, dt: deternment : nn:noun\n",
    "tagged=nltk.pos_tag(tokens1)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f426e",
   "metadata": {},
   "source": [
    "#  N_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1661561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is')\n",
      "('is', 'a')\n",
      "('a', 'very')\n",
      "('very', 'good')\n",
      "('good', 'book')\n",
      "('book', 'to')\n",
      "('to', 'study')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "sentance=\"this is a very good book to study\"\n",
    "# we are perform 2 gram\n",
    "ngram=ngrams(sequence=nltk.word_tokenize(sentance),n=2)\n",
    "for i in ngram:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c57e8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lorem', 'Ipsum', 'is')\n",
      "('Ipsum', 'is', 'simply')\n",
      "('is', 'simply', 'dummy')\n",
      "('simply', 'dummy', 'text')\n",
      "('dummy', 'text', 'of')\n",
      "('text', 'of', 'the')\n",
      "('of', 'the', 'printing')\n",
      "('the', 'printing', 'and')\n",
      "('printing', 'and', 'typesetting')\n",
      "('and', 'typesetting', 'industry')\n",
      "('typesetting', 'industry', '.')\n",
      "('industry', '.', 'Lorem')\n",
      "('.', 'Lorem', 'Ipsum')\n",
      "('Lorem', 'Ipsum', 'has')\n",
      "('Ipsum', 'has', 'been')\n",
      "('has', 'been', 'the')\n",
      "('been', 'the', 'industry')\n",
      "('the', 'industry', \"'s\")\n",
      "('industry', \"'s\", 'standard')\n",
      "(\"'s\", 'standard', 'dummy')\n",
      "('standard', 'dummy', 'text')\n",
      "('dummy', 'text', 'ever')\n",
      "('text', 'ever', 'since')\n",
      "('ever', 'since', 'the')\n",
      "('since', 'the', '1500s')\n",
      "('the', '1500s', ',')\n",
      "('1500s', ',', 'when')\n",
      "(',', 'when', 'an')\n",
      "('when', 'an', 'unknown')\n",
      "('an', 'unknown', 'printer')\n",
      "('unknown', 'printer', 'took')\n",
      "('printer', 'took', 'a')\n",
      "('took', 'a', 'galley')\n",
      "('a', 'galley', 'of')\n",
      "('galley', 'of', 'type')\n",
      "('of', 'type', 'and')\n",
      "('type', 'and', 'scrambled')\n",
      "('and', 'scrambled', 'it')\n",
      "('scrambled', 'it', 'to')\n",
      "('it', 'to', 'make')\n",
      "('to', 'make', 'a')\n",
      "('make', 'a', 'type')\n",
      "('a', 'type', 'specimen')\n",
      "('type', 'specimen', 'book')\n",
      "('specimen', 'book', '.')\n",
      "('book', '.', 'It')\n",
      "('.', 'It', 'has')\n",
      "('It', 'has', 'survived')\n",
      "('has', 'survived', 'not')\n",
      "('survived', 'not', 'only')\n",
      "('not', 'only', 'five')\n",
      "('only', 'five', 'centuries')\n",
      "('five', 'centuries', ',')\n",
      "('centuries', ',', 'but')\n",
      "(',', 'but', 'also')\n",
      "('but', 'also', 'the')\n",
      "('also', 'the', 'leap')\n",
      "('the', 'leap', 'into')\n",
      "('leap', 'into', 'electronic')\n",
      "('into', 'electronic', 'typesetting')\n",
      "('electronic', 'typesetting', ',')\n",
      "('typesetting', ',', 'remaining')\n",
      "(',', 'remaining', 'essentially')\n",
      "('remaining', 'essentially', 'unchanged')\n",
      "('essentially', 'unchanged', '.')\n",
      "('unchanged', '.', 'It')\n",
      "('.', 'It', 'was')\n",
      "('It', 'was', 'popularised')\n",
      "('was', 'popularised', 'in')\n",
      "('popularised', 'in', 'the')\n",
      "('in', 'the', '1960s')\n",
      "('the', '1960s', 'with')\n",
      "('1960s', 'with', 'the')\n",
      "('with', 'the', 'release')\n",
      "('the', 'release', 'of')\n",
      "('release', 'of', 'Letraset')\n",
      "('of', 'Letraset', 'sheets')\n",
      "('Letraset', 'sheets', 'containing')\n",
      "('sheets', 'containing', 'Lorem')\n",
      "('containing', 'Lorem', 'Ipsum')\n",
      "('Lorem', 'Ipsum', 'passages')\n",
      "('Ipsum', 'passages', ',')\n",
      "('passages', ',', 'and')\n",
      "(',', 'and', 'more')\n",
      "('and', 'more', 'recently')\n",
      "('more', 'recently', 'with')\n",
      "('recently', 'with', 'desktop')\n",
      "('with', 'desktop', 'publishing')\n",
      "('desktop', 'publishing', 'software')\n",
      "('publishing', 'software', 'like')\n",
      "('software', 'like', 'Aldus')\n",
      "('like', 'Aldus', 'PageMaker')\n",
      "('Aldus', 'PageMaker', 'including')\n",
      "('PageMaker', 'including', 'versions')\n",
      "('including', 'versions', 'of')\n",
      "('versions', 'of', 'Lorem')\n",
      "('of', 'Lorem', 'Ipsum')\n",
      "('Lorem', 'Ipsum', '.')\n"
     ]
    }
   ],
   "source": [
    "sentance1=\"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
    " Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "  when an unknown printer took a galley of type and scrambled it to make a type specimen book. \n",
    "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\n",
    " It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages,\n",
    "  and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\"\"\n",
    "ngram=ngrams(sequence=nltk.word_tokenize(sentance1),n=3)\n",
    "for i in ngram:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621305cc",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f98cfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem : lorem\n",
      "Ipsum : ipsum\n",
      "is : is\n",
      "simply : simpli\n",
      "dummy : dummi\n",
      "text : text\n",
      "of : of\n",
      "the : the\n",
      "printing : print\n",
      "and : and\n",
      "typesetting : typeset\n",
      "industry : industri\n",
      "It : it\n",
      "has : ha\n",
      "survived : surviv\n",
      "not : not\n",
      "only : onli\n",
      "five : five\n",
      "centuries : centuri\n",
      "but : but\n",
      "also : also\n",
      "the : the\n",
      "leap : leap\n",
      "into : into\n",
      "electronic : electron\n",
      "typesetting : typeset\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps=PorterStemmer()\n",
    "words=[\"Lorem\" ,\"Ipsum\" ,\"is\", \"simply\" ,\"dummy\", \"text\" ,\"of\", \"the\" ,\"printing\" ,\"and\" ,\"typesetting\" ,\"industry\",\"It\" ,\"has\", \"survived\" ,\"not\" ,\"only\", \"five\", \"centuries\", \"but\", \"also\" ,\"the\" ,\"leap\" ,\"into\" ,\"electronic\", \"typesetting\"]\n",
    "for i in words:\n",
    "    print(i,\":\",ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "466e0914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem : lorem\n",
      "Ipsum : ipsum\n",
      "is : is\n",
      "simply : simpli\n",
      "dummy : dummi\n",
      "text : text\n",
      "of : of\n",
      "the : the\n",
      "printing : print\n",
      "and : and\n",
      "typesetting : typeset\n",
      "industry : industri\n",
      "It : it\n",
      "has : has\n",
      "survived : surviv\n",
      "not : not\n",
      "only : onli\n",
      "five : five\n",
      "centuries : centuri\n",
      "but : but\n",
      "also : also\n",
      "the : the\n",
      "leap : leap\n",
      "into : into\n",
      "electronic : electron\n",
      "typesetting : typeset\n"
     ]
    }
   ],
   "source": [
    "# another stemmer is snowball stemmer\n",
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ss=SnowballStemmer(\"english\")\n",
    "words=[\"Lorem\" ,\"Ipsum\" ,\"is\", \"simply\" ,\"dummy\", \"text\" ,\"of\", \"the\" ,\"printing\" ,\"and\" ,\"typesetting\" ,\"industry\",\"It\" ,\"has\", \"survived\" ,\"not\" ,\"only\", \"five\", \"centuries\", \"but\", \"also\" ,\"the\" ,\"leap\" ,\"into\" ,\"electronic\", \"typesetting\"]\n",
    "for i in words:\n",
    "    print(i,\":\",ss.stem(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7128d",
   "metadata": {},
   "source": [
    "# Lemmitization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fca829",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a06666ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/anugrah/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/anugrah/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "718f82c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem : Lorem\n",
      "Ipsum : Ipsum\n",
      "is : is\n",
      "simply : simply\n",
      "dummy : dummy\n",
      "text : text\n",
      "of : of\n",
      "the : the\n",
      "printing : printing\n",
      "and : and\n",
      "typesetting : typesetting\n",
      "industry : industry\n",
      "It : It\n",
      "has : ha\n",
      "survived : survived\n",
      "not : not\n",
      "only : only\n",
      "five : five\n",
      "centuries : century\n",
      "but : but\n",
      "also : also\n",
      "the : the\n",
      "leap : leap\n",
      "into : into\n",
      "electronic : electronic\n",
      "typesetting : typesetting\n"
     ]
    }
   ],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "for i in words:\n",
    "    print(i,\":\",lem.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17a1f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better: good\n"
     ]
    }
   ],
   "source": [
    "print(\"better:\",lem.lemmatize(\"better\",pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c12bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
